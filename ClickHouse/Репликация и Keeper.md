---
tags:
  - db
  - clickhouse
---
![[Pasted image 20250426033037.png]]

**Без ZooKeeper/Keeper репликация не поддерживается ([Репликация данных | ClickHouse Docs](https://clickhouse.com/docs/ru/engines/table-engines/mergetree-family/replication#:~:text=match%20at%20L290%20%D0%95%D1%81%D0%BB%D0%B8%20ZooKeeper,%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D1%8B%20%D0%B1%D1%83%D0%B4%D1%83%D1%82%20%D1%82%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B4%D0%BB%D1%8F%20%D1%87%D1%82%D0%B5%D0%BD%D0%B8%D1%8F)). Поэтому для продакшена кластер обязательно должен иметь один из этих сервисов запущенным.**  если не вкл - то при старте станут “только для чтения” ([Репликация данных | ClickHouse Docs](https://clickhouse.com/docs/ru/engines/table-engines/mergetree-family/replication#:~:text=match%20at%20L290%20%D0%95%D1%81%D0%BB%D0%B8%20ZooKeeper,%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D1%8B%20%D0%B1%D1%83%D0%B4%D1%83%D1%82%20%D1%82%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B4%D0%BB%D1%8F%20%D1%87%D1%82%D0%B5%D0%BD%D0%B8%D1%8F)).

## Репликация MergeTree через ZooKeeper/Keeper

Механизм репликации в ClickHouse основан на централизованном **координаторе** – первоначально ZooKeeper, а в новых версиях появился встроенный аналог _ClickHouse Keeper_. Этот сервис отвечает за синхронизацию действий между репликами.

**Реплицируемые таблицы:** Как отмечалось ранее, существуют движки `ReplicatedMergeTree`[[ReplicatedMergeTree]] и его разновидности. При создании такой таблицы указываются два параметра:
- `zookeeper_path` – путь в кипере, где будут храниться метаданные таблицы (например, `/clickhouse/tables/{cluster}/{db}/{table}`).
- `replica_name` – имя реплики (уникальное имя текущего сервера/реплики, обычно используют hostname или идентификатор сервера).
Каждая нода при подъеме соединяется с кипере по заданному пути, регистрирует себя под своим именем и выполняет  _инициализацию_: Выполняет отстающие инструкции

**Роль кипера:**  хранит общую для всех реплик _очередь действий_ (реестр изменений). Когда на одной реплике выполняется INSERT, она:
1. Записывает вставленные части на свой диск.
2. Создает запись о новой части в реестре
3. Другие реплики через ZooKeeper видят новую задачу – появляется информация о новой part и где она есть.
4. Остальные реплики начинают копировать эту part (через внутренний протокол от реплики-источника) и по завершении подтверждают в ZK выполнение.
    

Короче, **все реплики проходят через одни и те же шаги в одном порядке**  ==Это обеспечивает _**Eventually** **консистентность**_: в итоге на всех репликах будет одинаковый набор частей.==

==ZooKeeper также следит за _состоянием реплик_: хранит список активных, их статус (активна, только чтение, мертва) и обеспечивает механизмы блокировок (например, чтобы две реплики не делали один и тот же merge одновременно – используется ZK lock).==

Настраивается он в конфигурации `<keeper>` – можно поднять несколько узлов Keeper для отказоустойчивости. С точки зрения таблиц, им все равно, Keeper или ZooKeeper – они используют один API. В документации рекомендуется использовать ClickHouse Keeper (начиная с версии 21.9) вместо отдельного ZooKeeper-кластера ([Репликация данных | ClickHouse Docs](https://clickhouse.com/docs/ru/engines/table-engines/mergetree-family/replication#:~:text=%D1%80%D0%B5%D0%BF%D0%BB%D0%B8%D0%BA%D0%B0%D1%85,%D0%BD%D0%BE%D0%B2%D0%B5%D0%B5%2C%20%D0%BD%D0%BE%20%D1%80%D0%B5%D0%BA%D0%BE%D0%BC%D0%B5%D0%BD%D0%B4%D1%83%D0%B5%D1%82%D1%81%D1%8F%20ClickHouse%20Keeper)), чтобы упростить инфраструктуру. Евгений Альбертович говорил о том что кипер предпочтительнее хотя бы даже по размеру занимаемых ресурсов собственной реализации в сравнении с ЗооКипером

Важно: если ZooKeeper/Keeper не настроен, вы не сможете создавать реплицированные таблицы (будет ошибка)

В то же время ClickHouse гарантирует отсутствие _дубликатов_ при корректно настроенных вставках: движок ReplacingMergeTree, например, полагается на то, что одна и та же часть с одним ID не будет дважды применена – кипер следит за уникальностью номеров вставленных блоков

Если реплика падает, она помечается как отвалившаяся, но это не блокирует работу других. Когда реплика возвращается, она читает из реестра всю очередь операций, накопленных за время оффлайна, и догоняет. Если какаято часть уже не доступна на других репликах (например, все удалили, а эта реплика пропустила), есть механизмы бэкапа (сохраняются старые части небольшое время). В крайнем случае, реплику можно переподключить с флагом `--force`, чтобы она заново скачала все с нуля.