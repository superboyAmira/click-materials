---
tags:
  - db
  - clickhouse
---
Архитектура клика изначально создавалась с упором на **масштабируемость и параллельную обработку** запросов. вычисления распараллеливаются по доступным ядрам. Таблицы могут быть распределены по разным узлам кластера (шардам), а на каждом шарде могут присутствовать реплики для отказоустойчивости. При выполнении одного запроса ClickHouse способен параллельно читать данные с нескольких _шардов_ и _реплик_, загружая все доступные ресурсы системы.

Кластер обычно строится по принципу **masterless (без единого ведущего узла)** – все узлы однородны и могут принимать запросы, _но как сказал Евгений Альбертович - может быть изменен в классический master-slave тип_. **Шардирование** и **репликация** настраиваются нами. Один запрос, поступивший на любой узел, может **проксираться** на другие узлы с данными: часть выполнения происходит удаленно, а результаты агрегации собираются и сводятся воедино. Например, в кластере с 2 шардами и 1 репликой на каждый шард распределенная таблица объединяет данные с обоих узлов; клиент может обращаться к ней как к единой таблице, а система сама разберет запрос по шардам и репликам параллельно.

Помимо горизонтального масштабирования по узлам, клик эффективно задействует многопоточность внутри одного узла. **Физическое хранение данных в виде независимых частей (part-ов)** позволяет выполнять фоновые слияния и очистку без блокировки чтения. На уровне одного запроса планировщик разбивает выполнение на потоки (например, сканирует разные куски таблицы в отдельных потоках) и затем объединяет результаты. Таким образом, достигается почти линейное ускорение на многопроцессорных серверах: чем больше ядер и узлов, тем больший объем данных можно обработать за единицу времени.


## Шарды и реплики: логическая организация кластера
![[Pasted image 20250425041123.png]]

Кластер ClickHouse определяется списком узлов, сгруппированных в **шарды**.
- **Шард (shard)** – это один или несколько узлов, хранящих определенную долю данных. Каждый шард содержит уникальную часть строк каждой распределенной таблицы (обычно по какому-то ключу). Шарды позволяют горизонтально масштабировать объем данных и пропускную способность: запросы могут выполняться параллельно на всех шардах. Шард традиционно соответствует _горизонтальной партиции_ данных.
- **Реплика (replica)** – узел, являющий копией другого узла (того же шарда). Все реплики одного шарда хранят _идентичный набор данных_. Репликация нужна для отказоустойчивости: если одна реплика недоступна, запросы могут обслуживаться другой. Также реплики могут распределять нагрузку на чтение (например, Distributed-таблица может слать запросы на разные реплики балансировано).
В конфигурации кластера обычно задается: `<remote_servers> -> cluster_name -> shards -> replicas`. Например: 2 шарда, по 2 реплики в каждом (4 узла итого). **Masterless:** у ClickHouse нет выделенного мастера – все реплики равноправны. Репликация реализуется по принципу «каждый пишет к себе и затем размножает по остальным через общий лог» (под контролем ZooKeeper).
**Распределенная таблица** (движок Distributed) объединяет шарды: она знает про все реплики каждого шарда. При SELECT запросы рассылаются по одному на шард (как правило, на одну из реплик шарда). При INSERT данные отправляются на все реплики нужного шарда.
